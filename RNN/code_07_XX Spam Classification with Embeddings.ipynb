{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdc90f29",
   "metadata": {},
   "source": [
    "## 07.01. Loading Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6332e262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded Data :\n",
      "------------------------------------\n",
      "  CLASS                                                SMS\n",
      "0   ham   said kiss, kiss, i can't do the sound effects...\n",
      "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
      "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
      "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n",
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3    spam\n",
      "4    spam\n",
      "Name: CLASS, dtype: object\n",
      "0     said kiss, kiss, i can't do the sound effects...\n",
      "1        &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
      "2    (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
      "3    * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
      "4    **FREE MESSAGE**Thanks for using the Auction S...\n",
      "Name: SMS, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "#Load Spam Data and review content\n",
    "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
    "\n",
    "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(spam_data.head())\n",
    "\n",
    "#Separate feature and target data\n",
    "spam_classes_raw = spam_data[\"CLASS\"]\n",
    "spam_messages = spam_data[\"SMS\"]\n",
    "print(spam_classes_raw.head())\n",
    "print(spam_messages.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e745a",
   "metadata": {},
   "source": [
    "## 07.02. Preprocessing Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e937480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before One Hot Encoding [0 0 1 ... 0 0 1]\n",
      "One-hot Encoding Shape :  (1500, 2)\n",
      "After One-Hot Encoding [[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Build a label encoder for target variable to convert strings to numeric values.\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "spam_classes = label_encoder.fit_transform(\n",
    "                                spam_classes_raw)\n",
    "print(\"Before One Hot Encoding\", spam_classes)\n",
    "\n",
    "#Convert target to one-hot encoding vector\n",
    "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
    "\n",
    "print(\"One-hot Encoding Shape : \", spam_classes.shape)\n",
    "print(\"After One-Hot Encoding\", spam_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a66f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens found:  4688\n",
      "Example token ID for word \"me\" : 25\n",
      "\n",
      "Total sequences found :  1500\n",
      "Example Sequence for sentence :   said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day! \n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  260  921  921    4  430   55    6 1488 2294  148   10\n",
      "    3 1489  464 1143  148  922   19  514   77 1144    3  515    1 2295\n",
      "  397   89]\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data for spam messages\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Max words in the vocabulary for this dataset\n",
    "VOCAB_WORDS=10000\n",
    "#Max sequence length for word sequences\n",
    "MAX_SEQUENCE_LENGTH=100\n",
    "\n",
    "#Create a vocabulary with unique words and IDs\n",
    "spam_tokenizer = Tokenizer(num_words=VOCAB_WORDS)\n",
    "spam_tokenizer.fit_on_texts(spam_messages)\n",
    "\n",
    "\n",
    "print(\"Total unique tokens found: \", len(spam_tokenizer.word_index))\n",
    "print(\"Example token ID for word \\\"me\\\" :\", spam_tokenizer.word_index.get(\"me\"))\n",
    "\n",
    "#Convert sentences to token-ID sequences\n",
    "spam_sequences = spam_tokenizer.texts_to_sequences(spam_messages)\n",
    "\n",
    "#Pad all sequences to fixed length\n",
    "spam_padded = pad_sequences(spam_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"\\nTotal sequences found : \", len(spam_padded))\n",
    "print(\"Example Sequence for sentence : \", spam_messages[0] )\n",
    "print(spam_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93cf70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                    spam_padded,spam_classes,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a2a40",
   "metadata": {},
   "source": [
    "## 07.03. Building the embeddding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea38967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size:  400000\n",
      "\n",
      " Sample Dictionary Entry for word \"the\" :\n",
      " [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "#Load the pre-trained embeddings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Read pretrained embeddings into a dictionary\n",
    "glove_dict = {} \n",
    "\n",
    "#Loading a 50 feature (dimension) embedding with 6 billion words\n",
    "with open('glove.6B.50d.txt', \"r\", encoding=\"utf8\") as glove_file:     \n",
    "    for line in glove_file:\n",
    "        \n",
    "        emb_line = line.split()      \n",
    "        emb_token = emb_line[0]         \n",
    "        emb_vector = np.array(emb_line[1:], dtype=np.float32)\n",
    "        \n",
    "        if emb_vector.shape[0] == 50:    \n",
    "            glove_dict[emb_token] = emb_vector \n",
    "\n",
    "print(\"Dictionary Size: \", len(glove_dict))\n",
    "print(\"\\n Sample Dictionary Entry for word \\\"the\\\" :\\n\", glove_dict.get(\"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da915726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Embedding matrix : (4689, 50)\n",
      "Embedding Vector for word \"me\" : \n",
      " [-0.14524999  0.31265     0.15184    -0.63708001  0.63552999 -0.50295001\n",
      " -0.23214     0.52891999 -0.58629     0.53934997 -0.3055      1.03569996\n",
      " -0.77989    -0.19386999  1.22150004  0.24521001  0.26144001  0.22439\n",
      "  0.15583999 -0.79145998 -0.65262002  1.3211      0.76617998  0.38234001\n",
      "  1.44529998 -2.26430011 -1.15050006  0.50373     1.2651     -1.59029996\n",
      "  3.05180001  0.84118003 -0.69542998  0.29984999 -0.49151    -0.22312\n",
      "  0.59527999 -0.076347    0.52358001 -0.50133997  0.22483     0.01546\n",
      " -0.088005    0.21281999  0.28545001 -0.15976    -0.16777    -0.50895\n",
      "  0.14322001  1.01180005]\n"
     ]
    }
   ],
   "source": [
    "#We now associate each token ID in our data set vocabulary to the corresponding embedding in Glove\n",
    "#If the word is not available, then embedding will be all zeros.\n",
    "\n",
    "#Matrix with 1 row for each word in the data set vocubulary and 50 features\n",
    "\n",
    "# get the count of our vocabulary (words extracted from SMSes)\n",
    "vocab_len = len(spam_tokenizer.word_index) + 1\n",
    "\n",
    "# initialize our embedding matrix to vocab_len X 50 with Zeroes\n",
    "# 50 is the count of features (dimensions) of embeddings that we imported in the previous step\n",
    "embedding_matrix = np.zeros((vocab_len, 50))\n",
    "\n",
    "for word, id in spam_tokenizer.word_index.items():  \n",
    "    try:\n",
    "        embedding_vector = glove_dict.get(word) \n",
    "        if embedding_vector is not None:         \n",
    "            embedding_matrix[id] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Size of Embedding matrix :\", embedding_matrix.shape)\n",
    "print(\"Embedding Vector for word \\\"me\\\" : \\n\", embedding_matrix[spam_tokenizer.word_index.get(\"me\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef5aff",
   "metadata": {},
   "source": [
    "## 07.04. Build the Spam Model with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3793aa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "4689\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Embedding-Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ ?                      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,450</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output-Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Embedding-Layer (\u001b[38;5;33mEmbedding\u001b[0m)     │ ?                      │       \u001b[38;5;34m234,450\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output-Layer (\u001b[38;5;33mDense\u001b[0m)            │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,450</span> (915.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m234,450\u001b[0m (915.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">234,450</span> (915.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m234,450\u001b[0m (915.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers import LSTM,Dense\n",
    "\n",
    "#Setup Hyper Parameters for building the model\n",
    "NB_CLASSES=2\n",
    "print(MAX_SEQUENCE_LENGTH)\n",
    "print(vocab_len)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(vocab_len,\n",
    "                                 50, \n",
    "                                 name=\"Embedding-Layer\",\n",
    "                                 weights=[embedding_matrix],\n",
    "                                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                 trainable=True))\n",
    "\n",
    "#Add LSTM Layer\n",
    "model.add(LSTM(256))\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5e339d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress:\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 327ms/step - accuracy: 0.5653 - loss: 0.6401 - val_accuracy: 0.9000 - val_loss: 0.3521\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step - accuracy: 0.8617 - loss: 0.3428 - val_accuracy: 0.9042 - val_loss: 0.3179\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 304ms/step - accuracy: 0.9298 - loss: 0.2667 - val_accuracy: 0.9250 - val_loss: 0.1884\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 325ms/step - accuracy: 0.9427 - loss: 0.1701 - val_accuracy: 0.9208 - val_loss: 0.2000\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 316ms/step - accuracy: 0.9270 - loss: 0.2103 - val_accuracy: 0.9333 - val_loss: 0.1565\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 356ms/step - accuracy: 0.9562 - loss: 0.1313 - val_accuracy: 0.9333 - val_loss: 0.1492\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 332ms/step - accuracy: 0.9514 - loss: 0.1286 - val_accuracy: 0.9417 - val_loss: 0.1532\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349ms/step - accuracy: 0.9525 - loss: 0.1401 - val_accuracy: 0.9417 - val_loss: 0.1426\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 340ms/step - accuracy: 0.9628 - loss: 0.1140 - val_accuracy: 0.9375 - val_loss: 0.1453\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 343ms/step - accuracy: 0.9695 - loss: 0.0917 - val_accuracy: 0.9375 - val_loss: 0.1335\n",
      "\n",
      "Evaluation against Test Dataset :\n",
      "------------------------------------\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 93ms/step - accuracy: 0.9566 - loss: 0.1523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15863721072673798, 0.9566666483879089]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make it verbose so we can see the progress\n",
    "VERBOSE=1\n",
    "\n",
    "#Setup Hyper Parameters for training\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=10\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
    "\n",
    "history=model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d8c3f7",
   "metadata": {},
   "source": [
    "## 07.05. Predicting Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aed75176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Prediction Output: [1 0]\n",
      "Prediction Classes are  ['spam' 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Two input strings to predict\n",
    "input_str=[\"Unsubscribe send GET EURO STOP to 83222\",\n",
    "            \"Yup I will come over\"]\n",
    "\n",
    "#Convert to sequence using the same tokenizer as training\n",
    "input_seq = spam_tokenizer.texts_to_sequences(input_str)\n",
    "#Pad the input\n",
    "input_padded = pad_sequences(input_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#Predict using model\n",
    "prediction=np.argmax( model.predict(input_padded), axis=1 )\n",
    "print(\"Prediction Output:\" , prediction)\n",
    "\n",
    "#Print prediction classes\n",
    "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc92c974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
